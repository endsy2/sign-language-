{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ]
  },
  {
   "cell_type": "code",
   "id": "3dfc61253fbea114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:57:49.626809600Z",
     "start_time": "2025-12-16T16:57:47.003801300Z"
    }
   },
   "source": "!pip install opencv-python numpy tensorflow scikit-learn matplotlib pillow pandas mediapipe ffmpeg-python",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: pillow in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: pandas in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: mediapipe in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (0.10.21)\n",
      "Collecting ffmpeg-python\n",
      "  Using cached ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (80.3.1)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: jax in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: jaxlib in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: opencv-contrib-python in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from mediapipe) (0.2.1)\n",
      "Collecting future (from ffmpeg-python)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: pycparser in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\university_folder\\signing language\\sign-language-\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Installing collected packages: future, ffmpeg-python\n",
      "\n",
      "   ---------------------------------------- 0/2 [future]\n",
      "   ---------------------------------------- 0/2 [future]\n",
      "   ---------------------------------------- 0/2 [future]\n",
      "   ---------------------------------------- 2/2 [ffmpeg-python]\n",
      "\n",
      "Successfully installed ffmpeg-python-0.2.0 future-1.0.0\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "50f7c3739b3e1829",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:08:26.243890700Z",
     "start_time": "2025-12-17T16:08:26.219820900Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "45d44d08cfdeb575",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T18:34:50.486989Z",
     "start_time": "2025-12-17T18:33:19.619239700Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "CLASSES = [\"Call\"]   # <-- Add more categories here\n",
    "SAVE_DIR = \"dataset\"\n",
    "VIDEO_COUNT = 15\n",
    "DURATION = 3\n",
    "PAUSE_DURATION = 3\n",
    "FPS = 30\n",
    "FRAME_WIDTH = 1280\n",
    "FRAME_HEIGHT = 720\n",
    "# -------------------------\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "cap.set(cv2.CAP_PROP_FPS, FPS)\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "print(\"Starting multi-category recording...\")\n",
    "\n",
    "for CLASS_NAME in CLASSES:\n",
    "\n",
    "    print(f\"\\n===== START CATEGORY: {CLASS_NAME} =====\")\n",
    "\n",
    "    # Create folders\n",
    "    class_folder = os.path.join(SAVE_DIR, CLASS_NAME)\n",
    "    os.makedirs(class_folder, exist_ok=True)\n",
    "    keypoints_folder = os.path.join(class_folder, \"keypoints\")\n",
    "    os.makedirs(keypoints_folder, exist_ok=True)\n",
    "\n",
    "    for i in range(1, VIDEO_COUNT + 1):\n",
    "\n",
    "        # CSV file for keypoints\n",
    "        csv_filename = f\"{CLASS_NAME}_{i}_keypoints.csv\"\n",
    "        csv_filepath = os.path.join(keypoints_folder, csv_filename)\n",
    "\n",
    "        csv_file = open(csv_filepath, \"w\", newline=\"\")\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # Header\n",
    "        header = []\n",
    "        for hand in ['left', 'right']:\n",
    "            for j in range(21):\n",
    "                header += [f\"{hand}_x{j}\", f\"{hand}_y{j}\", f\"{hand}_z{j}\"]\n",
    "        header.append(\"label\")\n",
    "        csv_writer.writerow(header)\n",
    "\n",
    "        # Video Writer\n",
    "        video_filename = f\"{CLASS_NAME}_{i}.mp4\"\n",
    "        video_path = os.path.join(class_folder, video_filename)\n",
    "        fourcc = cv4 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(video_path, fourcc, FPS, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "        print(f\"\\nRecording {CLASS_NAME} - Video {i}/{VIDEO_COUNT}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        while time.time() - start_time < DURATION:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Process with MediaPipe\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(rgb_frame)\n",
    "\n",
    "            left_hand_kp = [(0, 0, 0)] * 21\n",
    "            right_hand_kp = [(0, 0, 0)] * 21\n",
    "\n",
    "            if result.multi_hand_landmarks and result.multi_handedness:\n",
    "                for hand_landmarks, handedness in zip(result.multi_hand_landmarks, result.multi_handedness):\n",
    "                    label = handedness.classification[0].label\n",
    "                    hand_kp = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "\n",
    "                    if label == \"Left\":\n",
    "                        left_hand_kp = hand_kp\n",
    "                    else:\n",
    "                        right_hand_kp = hand_kp\n",
    "\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Save keypoints\n",
    "            row = [coord for kp in left_hand_kp + right_hand_kp for coord in kp]\n",
    "            row.append(CLASS_NAME)\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "            # Overlay info\n",
    "            sec_left = int(DURATION - (time.time() - start_time) + 1)\n",
    "            cv2.putText(frame, f\"Category: {CLASS_NAME}\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Recording Video {i}/{VIDEO_COUNT}\", (10, 80),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Time Left: {sec_left}s\", (10, 120),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # Show & write\n",
    "            cv2.imshow(\"Recorder\", frame)\n",
    "            out.write(frame)\n",
    "\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                print(\"Stopped by user.\")\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                raise SystemExit()\n",
    "\n",
    "        out.release()\n",
    "        csv_file.close()\n",
    "\n",
    "        print(f\"Saved video: {video_path}\")\n",
    "        print(f\"Saved keypoints: {csv_filepath}\")\n",
    "\n",
    "        # Pause between videos\n",
    "        print(f\"Waiting {PAUSE_DURATION} seconds...\")\n",
    "        pause_start = time.time()\n",
    "        while time.time() - pause_start < PAUSE_DURATION:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            remaining = int(PAUSE_DURATION - (time.time() - pause_start))\n",
    "            cv2.putText(frame, f\"Next video in {remaining}s\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "            cv2.imshow(\"Recorder\", frame)\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                raise SystemExit()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\\nAll categories recorded successfully!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting multi-category recording...\n",
      "\n",
      "===== START CATEGORY: Call =====\n",
      "\n",
      "Recording Call - Video 1/15\n",
      "Saved video: dataset\\Call\\Call_1.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_1_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 2/15\n",
      "Saved video: dataset\\Call\\Call_2.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_2_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 3/15\n",
      "Saved video: dataset\\Call\\Call_3.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_3_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 4/15\n",
      "Saved video: dataset\\Call\\Call_4.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_4_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 5/15\n",
      "Saved video: dataset\\Call\\Call_5.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_5_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 6/15\n",
      "Saved video: dataset\\Call\\Call_6.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_6_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 7/15\n",
      "Saved video: dataset\\Call\\Call_7.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_7_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 8/15\n",
      "Saved video: dataset\\Call\\Call_8.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_8_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 9/15\n",
      "Saved video: dataset\\Call\\Call_9.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_9_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 10/15\n",
      "Saved video: dataset\\Call\\Call_10.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_10_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 11/15\n",
      "Saved video: dataset\\Call\\Call_11.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_11_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 12/15\n",
      "Saved video: dataset\\Call\\Call_12.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_12_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 13/15\n",
      "Saved video: dataset\\Call\\Call_13.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_13_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 14/15\n",
      "Saved video: dataset\\Call\\Call_14.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_14_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Call - Video 15/15\n",
      "Saved video: dataset\\Call\\Call_15.mp4\n",
      "Saved keypoints: dataset\\Call\\keypoints\\Call_15_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "All categories recorded successfully!\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "2e8d133a20270740",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "81f819c577ab1d89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T18:34:55.865873800Z",
     "start_time": "2025-12-17T18:34:55.319026700Z"
    }
   },
   "source": [
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "DATASET_DIR = \"dataset\"    # folder with category subfolders\n",
    "SEQUENCE_LENGTH = 30       # number of frames per sample\n",
    "# -------------------------\n",
    "\n",
    "def fix_sequence_length(sequence, target_len):\n",
    "    \"\"\"Pad or truncate keypoint sequence to a fixed length.\"\"\"\n",
    "    if len(sequence) > target_len:\n",
    "        return sequence[:target_len]\n",
    "    elif len(sequence) < target_len:\n",
    "        pad = np.zeros((target_len - len(sequence), sequence.shape[1]))\n",
    "        return np.vstack([sequence, pad])\n",
    "    return sequence\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through categories\n",
    "for category in os.listdir(DATASET_DIR):\n",
    "    kp_dir = os.path.join(DATASET_DIR, category, \"keypoints\")\n",
    "    if not os.path.isdir(kp_dir):\n",
    "        continue\n",
    "\n",
    "    for csv_file in os.listdir(kp_dir):\n",
    "        csv_path = os.path.join(kp_dir, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Remove label column\n",
    "        keypoints = df.iloc[:, :-1].values.astype(np.float32)\n",
    "\n",
    "        # Fix sequence length\n",
    "        keypoints = fix_sequence_length(keypoints, SEQUENCE_LENGTH)\n",
    "\n",
    "        X.append(keypoints)\n",
    "        y.append(category)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (samples, SEQUENCE_LENGTH, 126)\n",
    "print(\"y shape:\", y.shape)  # (samples,)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "print(\"y_onehot shape:\", y_onehot.shape)\n",
    "\n",
    "# Save preprocessed data\n",
    "np.save(\"X_keypoints.npy\", X)\n",
    "np.save(\"y_labels.npy\", y_onehot)\n",
    "print(\"Preprocessed data saved to X_keypoints.npy and y_labels.npy\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (160, 30, 126)\n",
      "y shape: (160,)\n",
      "Classes: ['Call' 'Dad' 'Eat' 'Go' 'Good' 'Help' 'I' 'Love' 'No' 'Say' 'Stop' 'You']\n",
      "y_onehot shape: (160, 12)\n",
      "Preprocessed data saved to X_keypoints.npy and y_labels.npy\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "d52010923a03830b",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d02b7e9ea188379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T18:35:03.831438800Z",
     "start_time": "2025-12-17T18:34:58.119647600Z"
    }
   },
   "source": [
    "X = np.load(\"X_keypoints.npy\")      # shape: (samples, SEQUENCE_LENGTH, 126)\n",
    "y = np.load(\"y_labels.npy\")         # shape: (samples, num_classes)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Model definition\n",
    "# -------------------------\n",
    "SEQUENCE_LENGTH = X.shape[1]\n",
    "FEATURES = X.shape[2]\n",
    "NUM_CLASSES = y.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1D Convolutions over keypoints per frame\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(SEQUENCE_LENGTH, FEATURES)))\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM for temporal information\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# Callbacks\n",
    "# -------------------------\n",
    "checkpoint = ModelCheckpoint(\"sign_language_model.h5\", monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# -------------------------\n",
    "# Train-test split\n",
    "# -------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------------\n",
    "# Train the model\n",
    "# -------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (160, 30, 126)\n",
      "y shape: (160, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\University_Folder\\Signing Language\\sign-language-\\.venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_12\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_24 (\u001B[38;5;33mConv1D\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │        \u001B[38;5;34m24,256\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_25 (\u001B[38;5;33mConv1D\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m26\u001B[0m, \u001B[38;5;34m128\u001B[0m)        │        \u001B[38;5;34m24,704\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_12 (\u001B[38;5;33mMaxPooling1D\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m13\u001B[0m, \u001B[38;5;34m128\u001B[0m)        │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001B[38;5;33mDropout\u001B[0m)            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m13\u001B[0m, \u001B[38;5;34m128\u001B[0m)        │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_12 (\u001B[38;5;33mLSTM\u001B[0m)                  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │        \u001B[38;5;34m49,408\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001B[38;5;33mDropout\u001B[0m)            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m12\u001B[0m)             │           \u001B[38;5;34m780\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">780</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m99,148\u001B[0m (387.30 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">99,148</span> (387.30 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m99,148\u001B[0m (387.30 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">99,148</span> (387.30 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m9s\u001B[0m 1s/step - accuracy: 0.1875 - loss: 2.4381\n",
      "Epoch 1: val_accuracy improved from None to 0.06250, saving model to sign_language_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 46ms/step - accuracy: 0.1406 - loss: 2.4420 - val_accuracy: 0.0625 - val_loss: 2.5001\n",
      "Epoch 2/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 32ms/step - accuracy: 0.2500 - loss: 2.3899\n",
      "Epoch 2: val_accuracy improved from 0.06250 to 0.09375, saving model to sign_language_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.1719 - loss: 2.3637 - val_accuracy: 0.0938 - val_loss: 2.4730\n",
      "Epoch 3/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 30ms/step - accuracy: 0.1250 - loss: 2.3452\n",
      "Epoch 3: val_accuracy improved from 0.09375 to 0.15625, saving model to sign_language_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.2188 - loss: 2.2286 - val_accuracy: 0.1562 - val_loss: 2.3785\n",
      "Epoch 4/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - accuracy: 0.5000 - loss: 2.0234\n",
      "Epoch 4: val_accuracy did not improve from 0.15625\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.3125 - loss: 2.0609 - val_accuracy: 0.1562 - val_loss: 2.2673\n",
      "Epoch 5/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 33ms/step - accuracy: 0.1875 - loss: 2.1559\n",
      "Epoch 5: val_accuracy improved from 0.15625 to 0.34375, saving model to sign_language_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.3828 - loss: 1.8688 - val_accuracy: 0.3438 - val_loss: 2.0643\n",
      "Epoch 6/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 27ms/step - accuracy: 0.3125 - loss: 1.8635\n",
      "Epoch 6: val_accuracy did not improve from 0.34375\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.4531 - loss: 1.6657 - val_accuracy: 0.3438 - val_loss: 1.9708\n",
      "Epoch 7/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 33ms/step - accuracy: 0.5000 - loss: 1.5112\n",
      "Epoch 7: val_accuracy did not improve from 0.34375\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.4844 - loss: 1.5391 - val_accuracy: 0.2500 - val_loss: 1.9356\n",
      "Epoch 8/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 31ms/step - accuracy: 0.4375 - loss: 1.3009\n",
      "Epoch 8: val_accuracy did not improve from 0.34375\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.5234 - loss: 1.4160 - val_accuracy: 0.3438 - val_loss: 2.0012\n",
      "Epoch 9/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 31ms/step - accuracy: 0.5000 - loss: 1.4520\n",
      "Epoch 9: val_accuracy improved from 0.34375 to 0.43750, saving model to sign_language_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.5312 - loss: 1.3192 - val_accuracy: 0.4375 - val_loss: 1.7762\n",
      "Epoch 10/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 29ms/step - accuracy: 0.9375 - loss: 0.6529\n",
      "Epoch 10: val_accuracy improved from 0.43750 to 0.56250, saving model to sign_language_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6172 - loss: 1.1333 - val_accuracy: 0.5625 - val_loss: 1.5608\n",
      "Epoch 11/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 34ms/step - accuracy: 0.5000 - loss: 1.4161\n",
      "Epoch 11: val_accuracy did not improve from 0.56250\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.5938 - loss: 1.1264 - val_accuracy: 0.5000 - val_loss: 1.6915\n",
      "Epoch 12/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 32ms/step - accuracy: 0.6250 - loss: 1.0853\n",
      "Epoch 12: val_accuracy did not improve from 0.56250\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.6562 - loss: 1.0418 - val_accuracy: 0.5000 - val_loss: 1.5751\n",
      "Epoch 13/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 29ms/step - accuracy: 0.5625 - loss: 1.1795\n",
      "Epoch 13: val_accuracy did not improve from 0.56250\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.6250 - loss: 1.0804 - val_accuracy: 0.3750 - val_loss: 1.7338\n",
      "Epoch 14/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 26ms/step - accuracy: 0.8125 - loss: 0.6945\n",
      "Epoch 14: val_accuracy did not improve from 0.56250\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.6172 - loss: 0.9970 - val_accuracy: 0.5625 - val_loss: 1.5236\n",
      "Epoch 15/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 30ms/step - accuracy: 0.6875 - loss: 0.8320\n",
      "Epoch 15: val_accuracy did not improve from 0.56250\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.6953 - loss: 0.9237 - val_accuracy: 0.5625 - val_loss: 1.6008\n",
      "Epoch 16/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 26ms/step - accuracy: 0.6875 - loss: 0.9326\n",
      "Epoch 16: val_accuracy did not improve from 0.56250\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.6875 - loss: 0.9184 - val_accuracy: 0.4688 - val_loss: 1.6672\n",
      "Epoch 17/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 29ms/step - accuracy: 0.8125 - loss: 1.0939\n",
      "Epoch 17: val_accuracy improved from 0.56250 to 0.62500, saving model to sign_language_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.7188 - loss: 0.9215 - val_accuracy: 0.6250 - val_loss: 1.3815\n",
      "Epoch 18/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 27ms/step - accuracy: 0.7500 - loss: 0.7527\n",
      "Epoch 18: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.7344 - loss: 0.8042 - val_accuracy: 0.5938 - val_loss: 1.5110\n",
      "Epoch 19/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 31ms/step - accuracy: 0.6250 - loss: 1.2052\n",
      "Epoch 19: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.7500 - loss: 0.7809 - val_accuracy: 0.5312 - val_loss: 1.4221\n",
      "Epoch 20/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 29ms/step - accuracy: 0.8125 - loss: 0.7767\n",
      "Epoch 20: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.7266 - loss: 0.7911 - val_accuracy: 0.6250 - val_loss: 1.3626\n",
      "Epoch 21/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 27ms/step - accuracy: 0.8750 - loss: 0.4672\n",
      "Epoch 21: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.7812 - loss: 0.7474 - val_accuracy: 0.6250 - val_loss: 1.3825\n",
      "Epoch 22/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 31ms/step - accuracy: 0.9375 - loss: 0.4361\n",
      "Epoch 22: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.7656 - loss: 0.7288 - val_accuracy: 0.5938 - val_loss: 1.2742\n",
      "Epoch 23/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 29ms/step - accuracy: 0.8750 - loss: 0.5888\n",
      "Epoch 23: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.7734 - loss: 0.7354 - val_accuracy: 0.6250 - val_loss: 1.3360\n",
      "Epoch 24/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 29ms/step - accuracy: 0.8750 - loss: 0.4949\n",
      "Epoch 24: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.7500 - loss: 0.6718 - val_accuracy: 0.5625 - val_loss: 1.4068\n",
      "Epoch 25/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 28ms/step - accuracy: 0.9375 - loss: 0.3121\n",
      "Epoch 25: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.7422 - loss: 0.7351 - val_accuracy: 0.5938 - val_loss: 1.3758\n",
      "Epoch 26/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 27ms/step - accuracy: 0.8750 - loss: 0.6012\n",
      "Epoch 26: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.7812 - loss: 0.6262 - val_accuracy: 0.5000 - val_loss: 1.6579\n",
      "Epoch 27/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 30ms/step - accuracy: 0.8750 - loss: 0.4206\n",
      "Epoch 27: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.7656 - loss: 0.6993 - val_accuracy: 0.5938 - val_loss: 1.4427\n",
      "Epoch 28/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 28ms/step - accuracy: 0.8125 - loss: 0.5710\n",
      "Epoch 28: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.6953 - loss: 0.7877 - val_accuracy: 0.5625 - val_loss: 1.6276\n",
      "Epoch 29/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 30ms/step - accuracy: 0.6250 - loss: 0.8478\n",
      "Epoch 29: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.7891 - loss: 0.6289 - val_accuracy: 0.5938 - val_loss: 1.3142\n",
      "Epoch 30/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 27ms/step - accuracy: 0.7500 - loss: 0.6515\n",
      "Epoch 30: val_accuracy did not improve from 0.62500\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.8047 - loss: 0.5544 - val_accuracy: 0.6250 - val_loss: 1.3117\n",
      "Epoch 31/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 29ms/step - accuracy: 0.9375 - loss: 0.2215\n",
      "Epoch 31: val_accuracy improved from 0.62500 to 0.65625, saving model to sign_language_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8594 - loss: 0.4659 - val_accuracy: 0.6562 - val_loss: 1.2930\n",
      "Epoch 32/50\n",
      "\u001B[1m1/8\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 27ms/step - accuracy: 0.8125 - loss: 0.4079\n",
      "Epoch 32: val_accuracy did not improve from 0.65625\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.8125 - loss: 0.5200 - val_accuracy: 0.6562 - val_loss: 1.3442\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "cac3c842dbe6fc13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T18:38:10.089013400Z",
     "start_time": "2025-12-17T18:35:11.507724800Z"
    }
   },
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "MODEL_PATH = \"sign_language_model.h5\"   # your trained model\n",
    "DATASET_DIR = \"dataset\"                 # folder used in preprocessing\n",
    "SEQUENCE_LENGTH = 30\n",
    "FRAME_WIDTH = 1280\n",
    "FRAME_HEIGHT = 720\n",
    "FPS = 30\n",
    "# -------------------------\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Dynamically get class names from dataset folder\n",
    "CLASSES = [d for d in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, d))]\n",
    "CLASSES.sort()  # ensure consistent order\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "cap.set(cv2.CAP_PROP_FPS, FPS)\n",
    "\n",
    "# Sequence buffer\n",
    "sequence = []\n",
    "\n",
    "print(\"Starting real-time gesture test. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # MediaPipe processing\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # Extract keypoints\n",
    "    left_hand_kp = [(0, 0, 0)] * 21\n",
    "    right_hand_kp = [(0, 0, 0)] * 21\n",
    "\n",
    "    if result.multi_hand_landmarks and result.multi_handedness:\n",
    "        for hand_landmarks, handedness in zip(result.multi_hand_landmarks, result.multi_handedness):\n",
    "            label = handedness.classification[0].label\n",
    "            hand_kp = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "            if label == \"Left\":\n",
    "                left_hand_kp = hand_kp\n",
    "            else:\n",
    "                right_hand_kp = hand_kp\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Flatten keypoints and add to sequence\n",
    "    keypoints = [coord for kp in left_hand_kp + right_hand_kp for coord in kp]\n",
    "    sequence.append(keypoints)\n",
    "\n",
    "    # Keep last SEQUENCE_LENGTH frames\n",
    "    if len(sequence) > SEQUENCE_LENGTH:\n",
    "        sequence = sequence[-SEQUENCE_LENGTH:]\n",
    "\n",
    "    # Make prediction when sequence is full\n",
    "    if len(sequence) == SEQUENCE_LENGTH:\n",
    "        input_data = np.expand_dims(sequence, axis=0)  # shape: (1, SEQUENCE_LENGTH, 126)\n",
    "        prediction = model.predict(input_data, verbose=0)\n",
    "        class_id = np.argmax(prediction)\n",
    "        class_name = CLASSES[class_id]\n",
    "        confidence = prediction[0][class_id]\n",
    "        cv2.putText(frame, f\"{class_name} ({confidence*100:.1f}%)\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Real-Time Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time gesture test. Press 'q' to quit.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[58], line 76\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sequence) \u001B[38;5;241m==\u001B[39m SEQUENCE_LENGTH:\n\u001B[0;32m     75\u001B[0m     input_data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(sequence, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# shape: (1, SEQUENCE_LENGTH, 126)\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m     prediction \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m     class_id \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(prediction)\n\u001B[0;32m     78\u001B[0m     class_name \u001B[38;5;241m=\u001B[39m CLASSES[class_id]\n",
      "File \u001B[1;32mD:\\University_Folder\\Signing Language\\sign-language-\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a833ff418976f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
